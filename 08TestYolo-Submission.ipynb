{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e1d9561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8cd1fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yolov5',\n",
       " 'FreeMono.ttf',\n",
       " '06TrainYolo.ipynb',\n",
       " '07TestYolo.ipynb',\n",
       " '.DS_Store',\n",
       " '04DataSplitting.ipynb',\n",
       " 'dataRoad.yaml',\n",
       " '08TestYolo-Submission.ipynb',\n",
       " '03CheckAnnotations.ipynb',\n",
       " '02MakeAnnotations.ipynb',\n",
       " 'dataset',\n",
       " 'CreateAnnotations.ipynb',\n",
       " '01CheckGivenBoundingBoxes.ipynb',\n",
       " '05GetInstallYolo.ipynb',\n",
       " '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cd10ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets copy all test files to a folder\n",
    "\n",
    "df_test=pd.read_csv(\"dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf3b41c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2092, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7091ee7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>953ab1447c46ecfef67ab14629cd70c7.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e4ddbaa7970fca225a51288ce5f7d3f9.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5b8120d69607a077b7583334be3ba18b.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138b1dc82005b4c33e4886260649d313.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0f91ec1533b845b13089f8cf4e0a36f7.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             image_path\n",
       "0  953ab1447c46ecfef67ab14629cd70c7.jpg\n",
       "1  e4ddbaa7970fca225a51288ce5f7d3f9.jpg\n",
       "2  5b8120d69607a077b7583334be3ba18b.jpg\n",
       "3  138b1dc82005b4c33e4886260649d313.jpg\n",
       "4  0f91ec1533b845b13089f8cf4e0a36f7.jpg"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8311c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(os.path.join(\"dataset\",\"test_submit_images\")):\n",
    "    os.mkdir(os.path.join(\"dataset\",\"test_submit_images\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cb54917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f in list(df_test[\"image_path\"].unique()):\n",
    "#     shutil.copy(os.path.join('dataset','images',f),os.path.join('dataset','test_submit_images'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9bbe1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python yolov5/detect.py --source dataset/test_submit_images/ --weights yolov5/runs/train/RoadTrainModel_yx_50_2/weights/best.pt --conf 0.25 --name RoadTestModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65202188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/linux/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2023-1-18 Python-3.8.15 torch-1.13.1 CUDA:0 (NVIDIA GeForce RTX 2080 with Max-Q Design, 7966MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 212 layers, 20893344 parameters, 0 gradients, 48.0 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', \"/home/linux/Desktop/Smartathon/Problem 1/yolov5/runs/train/RoadTrainModel_m_100/weights/best.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c454510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images\n",
    "# test_files=os.listdir(\"dataset/test_submit_images/\")[:250]\n",
    "test_files=os.listdir(\"dataset/test_submit_images/\")\n",
    "imgs=[]\n",
    "for tfile in test_files:\n",
    "    imgs.append(Image.open(os.path.join(\"dataset/test_submit_images/\",tfile)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53ba085c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.75 GiB (GPU 0; 7.78 GiB total capacity; 2.96 GiB already allocated; 2.45 GiB free; 4.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_cuda/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_cuda/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:705\u001b[0m, in \u001b[0;36mAutoShape.forward\u001b[0;34m(self, ims, size, augment, profile)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mautocast(autocast):\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dt[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 705\u001b[0m         y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;66;03m# Post-process\u001b[39;00m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dt[\u001b[38;5;241m2\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_cuda/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:515\u001b[0m, in \u001b[0;36mDetectMultiBackend.forward\u001b[0;34m(self, im, augment, visualize)\u001b[0m\n\u001b[1;32m    512\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m--> 515\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize) \u001b[38;5;28;01mif\u001b[39;00m augment \u001b[38;5;129;01mor\u001b[39;00m visualize \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:  \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_cuda/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/yolo.py:209\u001b[0m, in \u001b[0;36mDetectionModel.forward\u001b[0;34m(self, x, augment, profile, visualize)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_augment(x)  \u001b[38;5;66;03m# augmented inference, None\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/yolo.py:121\u001b[0m, in \u001b[0;36mBaseModel._forward_once\u001b[0;34m(self, x, profile, visualize)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 121\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    122\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_cuda/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:60\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_cuda/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_cuda/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_cuda/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.75 GiB (GPU 0; 7.78 GiB total capacity; 2.96 GiB already allocated; 2.45 GiB free; 4.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "results = model(imgs)  # includes NMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8c133bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2092"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3465134b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Results\n",
    "# results.print()  # print results to screen\n",
    "# results.show()  # display results\n",
    "# results.save()  # save as results1.jpg, results2.jpg... etc.\n",
    "\n",
    "# # Data\n",
    "print('\\n', results.xyxy[0])  # print img1 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e17432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in results.pandas().xyxy:\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32703860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tensor([[5.66013e+02, 4.66487e+02, 8.34076e+02, 6.98943e+02, 5.27743e-01, 2.00000e+00],\n",
      "        [1.01827e+03, 5.81633e+02, 1.34726e+03, 9.72367e+02, 5.26731e-01, 2.00000e+00],\n",
      "        [4.65080e+02, 5.32425e+02, 7.51274e+02, 7.86035e+02, 4.18262e-01, 2.00000e+00],\n",
      "        [3.31445e+02, 6.04886e+02, 6.73528e+02, 9.00779e+02, 2.81872e-01, 2.00000e+00]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print('\\n', results.xyxy[0])  # print img1 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56293fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b495b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.xyxy[0]  # im1 predictions (tensor)\n",
    "results.pandas().xyxy[0] # im1 predictions (pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6a37a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f7943ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv(\"dataset/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a3a4a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>image_path</th>\n",
       "      <th>name</th>\n",
       "      <th>xmax</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymax</th>\n",
       "      <th>ymin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4a48c42c9579ec0399e6c5a3e825e765.jpg</td>\n",
       "      <td>GARBAGE</td>\n",
       "      <td>797.0</td>\n",
       "      <td>701.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>211.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4a48c42c9579ec0399e6c5a3e825e765.jpg</td>\n",
       "      <td>GARBAGE</td>\n",
       "      <td>932.0</td>\n",
       "      <td>786.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>238.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4a48c42c9579ec0399e6c5a3e825e765.jpg</td>\n",
       "      <td>GARBAGE</td>\n",
       "      <td>736.0</td>\n",
       "      <td>657.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>229.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>ea906a663da6321bcef78be4b7d1afff.jpg</td>\n",
       "      <td>BAD_BILLBOARD</td>\n",
       "      <td>986.0</td>\n",
       "      <td>786.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1c7d48005a12d1b19261b8e71df7cafe.jpg</td>\n",
       "      <td>SAND_ON_ROAD</td>\n",
       "      <td>667.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>179.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                            image_path           name   xmax   xmin  \\\n",
       "0    3.0  4a48c42c9579ec0399e6c5a3e825e765.jpg        GARBAGE  797.0  701.0   \n",
       "1    3.0  4a48c42c9579ec0399e6c5a3e825e765.jpg        GARBAGE  932.0  786.0   \n",
       "2    3.0  4a48c42c9579ec0399e6c5a3e825e765.jpg        GARBAGE  736.0  657.0   \n",
       "3    7.0  ea906a663da6321bcef78be4b7d1afff.jpg  BAD_BILLBOARD  986.0  786.0   \n",
       "4    8.0  1c7d48005a12d1b19261b8e71df7cafe.jpg   SAND_ON_ROAD  667.0  549.0   \n",
       "\n",
       "    ymax   ymin  \n",
       "0  262.0  211.0  \n",
       "1  329.0  238.0  \n",
       "2  275.0  229.0  \n",
       "3  136.0    0.0  \n",
       "4  228.0  179.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "538af2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>image_path</th>\n",
       "      <th>name</th>\n",
       "      <th>xmax</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymax</th>\n",
       "      <th>ymin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4a48c42c9579ec0399e6c5a3e825e765.jpg</td>\n",
       "      <td>GARBAGE</td>\n",
       "      <td>797.0</td>\n",
       "      <td>701.0</td>\n",
       "      <td>262.0</td>\n",
       "      <td>211.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4a48c42c9579ec0399e6c5a3e825e765.jpg</td>\n",
       "      <td>GARBAGE</td>\n",
       "      <td>932.0</td>\n",
       "      <td>786.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>238.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4a48c42c9579ec0399e6c5a3e825e765.jpg</td>\n",
       "      <td>GARBAGE</td>\n",
       "      <td>736.0</td>\n",
       "      <td>657.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>229.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>ea906a663da6321bcef78be4b7d1afff.jpg</td>\n",
       "      <td>BAD_BILLBOARD</td>\n",
       "      <td>986.0</td>\n",
       "      <td>786.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1c7d48005a12d1b19261b8e71df7cafe.jpg</td>\n",
       "      <td>SAND_ON_ROAD</td>\n",
       "      <td>667.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>179.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                            image_path           name   xmax   xmin  \\\n",
       "0    3.0  4a48c42c9579ec0399e6c5a3e825e765.jpg        GARBAGE  797.0  701.0   \n",
       "1    3.0  4a48c42c9579ec0399e6c5a3e825e765.jpg        GARBAGE  932.0  786.0   \n",
       "2    3.0  4a48c42c9579ec0399e6c5a3e825e765.jpg        GARBAGE  736.0  657.0   \n",
       "3    7.0  ea906a663da6321bcef78be4b7d1afff.jpg  BAD_BILLBOARD  986.0  786.0   \n",
       "4    8.0  1c7d48005a12d1b19261b8e71df7cafe.jpg   SAND_ON_ROAD  667.0  549.0   \n",
       "\n",
       "    ymax   ymin  \n",
       "0  262.0  211.0  \n",
       "1  329.0  238.0  \n",
       "2  275.0  229.0  \n",
       "3  136.0    0.0  \n",
       "4  228.0  179.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bde2ed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "names=list(df_train[\"name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e093b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GARBAGE [3.0]\n",
      "BAD_BILLBOARD [7.0]\n",
      "SAND_ON_ROAD [8.0]\n",
      "GRAFFITI [0.0]\n",
      "POTHOLES [2.0]\n",
      "CLUTTER_SIDEWALK [9.0]\n",
      "CONSTRUCTION_ROAD [4.0]\n",
      "BROKEN_SIGNAGE [5.0]\n",
      "UNKEPT_FACADE [10.0]\n",
      "FADED_SIGNAGE [1.0]\n",
      "BAD_STREETLIGHT [6.0]\n"
     ]
    }
   ],
   "source": [
    "mapping_dic_name_class={}\n",
    "mapping_dic_class_name={}\n",
    "for name in names:\n",
    "    df_name=df_train[df_train[\"name\"]==name]\n",
    "    classes=list(df_name[\"class\"].unique())\n",
    "    print(name,classes)\n",
    "    mapping_dic_name_class[name]=classes[0]\n",
    "    mapping_dic_class_name[classes[0]]=name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd3849",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_df={}\n",
    "dic_df[\"class\"]=[]\n",
    "dic_df[\"image_path\"]=[]\n",
    "dic_df[\"name\"]=[]\n",
    "dic_df[\"xmax\"]=[]\n",
    "dic_df[\"xmin\"]=[]\n",
    "dic_df[\"ymax\"]=[]\n",
    "dic_df[\"ymin\"]=[]\n",
    "\n",
    "for i in range(len(results.pandas().xyxy)):\n",
    "    img_name=test_files[i]\n",
    "    df=results.pandas().xyxy[i]\n",
    "    print(img_name,\"\\n\",df)\n",
    "    for index,row in df.iterrows():\n",
    "        xmin=int(row[\"xmin\"])//2\n",
    "        ymin=int(row[\"ymin\"])//2\n",
    "        xmax=int(row[\"xmax\"])//2\n",
    "        ymax=int(row[\"ymax\"])//2\n",
    "        class_=row[\"class\"]\n",
    "        name=row[\"name\"]\n",
    "        class_name=mapping_dic_class_name[float(name)]\n",
    "        dic_df[\"class\"].append(name)\n",
    "        dic_df[\"image_path\"].append(img_name)\n",
    "        dic_df[\"name\"].append(class_name)\n",
    "        dic_df[\"xmax\"].append(xmax)\n",
    "        dic_df[\"xmin\"].append(xmin)\n",
    "        dic_df[\"ymax\"].append(ymax)\n",
    "        dic_df[\"ymin\"].append(ymin)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ae041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_df_res=pd.DataFrame(dic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790fce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_df_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadaf470",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_df_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b50b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_df_res.to_csv(\"dataset/test_submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33791aff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_kuda_kernel",
   "language": "python",
   "name": "torch_kuda_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
